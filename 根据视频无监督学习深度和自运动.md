

[toc]

# Unsupervised Learning of Depth and Ego-Motion from Video

* 目的：用视频序列估计单目深度和相机运动
* 模型：无监督学习
* 数据集：KITTI
* 输入：图像序列（视频）
* 输出：单目深度和位姿估计
* 前提：相机内参已知

## 一、介绍

人类的建模能力来自于对世界的多次观察。通过训练一个观察图像序列的模型来模仿这种能力。采用端到端的方法，允许模型直接从输入像素映射到自运动的估计。

此方法无需手动标记，也无需相机运动信息。

## 二、相关工作

#### 从运动恢复结构（SFM）

传统工具链对精确图像的依赖可能导致低纹理、复杂几何/光度、薄结构或遮挡等问题。

使用深度学习，如特征匹配、位姿估计、立体图，有克服以上问题的潜力。

#### 基于扭曲的视图合成

几何场景理解的一个重要应用是新视图合成。

目标是从经过训练的 CNN 中提取几何推理能力，来执行基于扭曲的视图合成。

#### 从注册的2D视图学习单视图3D

大多数技术集中在推断深度图作为场景几何输出，最近的研究表明，通过利用场景规律性，甚至可以在无3D标签的情况下学习3D推理。

#### 从视频中进行无监督/自监督学习

从视频中学习通用视觉特征，稍后可用于其他视觉任务，如对象检测和语义分割。

 Vijayanarasimhan 等提出从视频中联合训练深度、相机运动和场景运动，增加了合并监督的能力。

## 三、方法

提出 从未标记的视频序列中联合训练单视图深度CNN和相机位姿估计CNN。

### 3.1 用综合作监督

给定场景的一个输入视图，合成从不同相机位姿看到的场景的新图像。给定图像中每像素深度，加上附近视图的位姿和可见性，就可合成目标视图。

**取中间一帧的图像作为 Depth CNN 的输入，输出此帧的预测深度图；取前后相邻帧作为 Pose CNN 的输入，输出对相机运动的位姿预测；将预测的深度图映射到运动轨迹得到对原中间帧的预测图，将预测帧与原帧的差别作为损失函数。**

### 3.2 可微分的基于图像的深度渲染

学习框架的关键组件是可微分的基于图像的深度渲染器。

双线性插值（bilinear sampling）

### 3.3 建模模型限制

* 场景为静态，无移动对象
* 目标视图与原视图之间无遮挡
* 表面是Lambertian的，因此光一致性误差有意义

### 3.4 克服梯度局部化

梯度来自原视图与预测的4个临近位置的值，若正确值存在与低纹理区或离采样点较远，会抑制训练过程。

两种策略：

1. 使用具有小瓶颈的卷积编码/解码器构建深度估计网络
2. 显式的多尺度和平滑度损失，允许从较大的空间区域获得梯度

使用后者因为其对架构选择不敏感。

### 3.5 网络架构

#### 单视图深度

采用 DispNet 架构，除预测层外，所有卷积层都使用 ReLU 激活函数。

#### 位姿

全局平均池化应用于所有空间位置的聚合预测。除了最后一层没有应用非线性激活，所有的卷积层都使用 ReLU 激活函数。

#### 可解释性掩模

可解释性预测网络与位姿网络共享前五个特征编码层，除了没有非线性激活的预测层之外，所有的卷积/反卷积层都使用 ReLU 激活。

## 四、实验

使用 KITTI 数据集进行基准测试，使用 Make3D 数据集评估泛化能力。使用开源的 TensorFlow 架构、CUDA 8、Ubuntu 16.04.

## 五、议论

未解决的问题：

1. 当前框架没有明确估计场景动态和遮挡
2. 模型假设相机内参已知
3. 深度图是基础3D场景的简化表示

展望：

1. 位姿网络用某种形式的图像对应估计相机运动
2. 深度估计网络识别场景和物体的共同结构特征
3. 将此网络用于对象检测和语义分割...
